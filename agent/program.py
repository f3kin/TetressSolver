# COMP30024 Artificial Intelligence, Semester 1 2024
# Project Part B: Game Playing Agent

MID_GAME = 2
OPENING = 1
END_GAME = 3

from referee.game import PlayerColor, Action, PlaceAction, Coord


class Agent:
    """
    This class is the "entry point" for your agent, providing an interface to
    respond to various Tetress game events.
    """

    def __init__(self, color: PlayerColor, **referee: dict):
        """
        This constructor method runs when the referee instantiates the agent.
        Any setup and/or precomputation should be done here.
        """
        # TODO: IMPLEMENT PRECOMPUTATION HERE
        self._color = color
        state = OPENING #TODO: make this visible in action
        num_moves = 0 #TODO: make this visible in action, update
        book_moves = [PlaceAction( #TODO: make these moves legitimate, will have to be slightly more complex, i.e. first red is open, blue is always a response to the red book move
                    Coord(3, 3), 
                    Coord(3, 4), 
                    Coord(4, 3), 
                    Coord(4, 4)
                ), PlaceAction(
                    Coord(3, 3), 
                    Coord(3, 4), 
                    Coord(4, 3), 
                    Coord(4, 4)
                ), PlaceAction(
                    Coord(3, 3), 
                    Coord(3, 4), 
                    Coord(4, 3), 
                    Coord(4, 4)
                )]
        match color:
            case PlayerColor.RED:
                print("Testing: I am playing as RED")
            case PlayerColor.BLUE:
                print("Testing: I am playing as BLUE")

    def action(self, **referee: dict) -> Action:
        """
        This method is called by the referee each time it is the agent's turn
        to take an action. It must always return an action object. 
        """

        # Below we have hardcoded two actions to be played depending on whether
        # the agent is playing as BLUE or RED. Obviously this won't work beyond
        # the initial moves of the game, so you should use some game playing
        # technique(s) to determine the best action to take.
        #TODO: PUT ACTION CHOOSING LOGIC
        if (num_moves > 3): #TODO: Make this state logic more complex
            if (num_moves > 140):
                state = END_GAME
            else:
                state = MID_GAME
        match self.state:
            case OPENING:
                return book_moves[num_moves]
            case MID_GAME:
                return search(board, self._color)
            case END_GAME:
                return endgame_search(board, self._color)



    def update(self, color: PlayerColor, action: Action, **referee: dict):
        """
        This method is called by the referee after an agent has taken their
        turn. You should use it to update the agent's internal game state. 
        """

        # There is only one action type, PlaceAction
        place_action: PlaceAction = action
        c1, c2, c3, c4 = place_action.coords
        num_moves += 1
        # Here we are just printing out the PlaceAction coordinates for
        # demonstration purposes. You should replace this with your own logic
        # to update your agent's internal game state representation.
        print(f"Testing: {color} played PLACE action: {c1}, {c2}, {c3}, {c4}")

###### Functions specific to Minimax #####
def search(board, colour):
    return minimax(board, colour) # Returns the placeaction of the best move to make

def minimax(state, game, a, b):
    if cutoff_test(state):
        return evaluation(state)
    return max_value(state, game, a, b) # Assumes we are always maximising, think this is correct ?
    
def max_value(state, game, a, b):
    # State - current game state (probably the board)
    # Game - game description
    # a - the best score for max along the path to state
    # b - the best score for min along the path to state
    if cutoff_test(state):
        return evaluation(state)
    children = expand(state)
    for child in children:
        a = max(a, min_value(child, game, a, b))
        if a >= b:
            return b
    return a

def min_value(state, game, a ,b):
    if cutoff_test(state):
        return evaluation(state)
    children = expand(state)
    for child in children:
        b = min(b,max_value(child, game, a, b))
        if b <= a:
            return a
    return b

# Will evaluate a board state and assign it a value
def evaluation(board, colour):
    # will be generated by Nic in his own branch, returns an integer score for the node where higher is better
    return 0

# Checks if the move is a completed game(very unlikely)
def cutoff_test(state):
    #Unsure if this is both for winning and losing
    return 0

#returns the best move based on an end game scenario
def endgame_search(board, color):
    # May not be needed as we could simply modify the heuristic 
    return PlaceAction(
                    Coord(3, 3), 
                    Coord(3, 4), 
                    Coord(4, 3), 
                    Coord(4, 4)
                )